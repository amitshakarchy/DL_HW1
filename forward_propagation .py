"""

"""


def initialize_parameters(layer_dims):
    """
    input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)
    output: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively
    """
    pass


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return:
        Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
        linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    pass


def softmax(Z):
    """
    note:
    Softmax can be thought of as a sigmoid for multi-class problems. The formula for softmax for each node in the output layer is as follows:
    Softmax〖(z)〗_i=(exp⁡(z_i))/(∑_j▒〖exp⁡(z_j)〗)

    :param Z: the linear component of the activation function
    :return:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation
    """
    pass


def relu(Z):
    """

    :param Z: the linear component of the activation function
    :return:
        A – the activations of the layer
        activation_cache – returns Z, which will be useful for the backpropagation

    """
    pass


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return:
        A – the activations of the current layer
        cache – a joint dictionary containing both linear_cache and activation_cache
    """
    pass


def L_model_forward(X, parameters, use_batchnorm):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
        (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :return:
        AL – the last post-activation value
        caches – a list of all the cache objects generated by the linear_forward function

    """
    pass


def compute_cost(AL, Y):
    """
    	Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss. The formula is as follows :
        cost=-1/m*∑_1^m▒∑_1^C▒〖y_i  log⁡〖(y ̂)〗 〗, where y_i is one for the true class (“ground-truth”) and y ̂ is the softmax-adjusted prediction
        (this link provides a good overview).

    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return:
        cost – the cross-entropy cost
    """
    pass


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return:
        NA - the normalized activation values, based on the formula learned in class
    """
    pass
